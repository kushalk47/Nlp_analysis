{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas \n",
    "#!pip install nltk\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0  The worst experience ever! My car stopped in t...   \n",
      "1  The parts were defective, and the dealer did n...   \n",
      "2  The dealer insulted me when I asked about the ...   \n",
      "3  The worst experience ever! My car stopped in t...   \n",
      "4  I was scammed by the dealer, and they refused ...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  the worst experience ever my car stopped in th...  \n",
      "1  the parts were defective and the dealer did no...  \n",
      "2  the dealer insulted me when i asked about the ...  \n",
      "3  the worst experience ever my car stopped in th...  \n",
      "4  i was scammed by the dealer and they refused t...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset (Replace with actual file path)\n",
    "df = pd.read_csv(\"synthetic.csv\")  # Change to your file path\n",
    "\n",
    "# Drop rows where 'Review' is missing\n",
    "df = df.dropna(subset=['Review'])\n",
    "\n",
    "# Convert text to lowercase and remove special characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure it's a string\n",
    "        text = text.lower()  # Lowercasing\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "        return text.strip()\n",
    "    return \"\"\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_review'] = df['Review'].apply(clean_text)\n",
    "\n",
    "# Show first few rows to verify\n",
    "print(df[['Review', 'cleaned_review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have downloaded the ntlk lib from that now we are installing stopwords and lemmatizier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  \\\n",
      "0  the worst experience ever my car stopped in th...   \n",
      "1  the parts were defective and the dealer did no...   \n",
      "2  the dealer insulted me when i asked about the ...   \n",
      "3  the worst experience ever my car stopped in th...   \n",
      "4  i was scammed by the dealer and they refused t...   \n",
      "\n",
      "                                    processed_review  \n",
      "0  the worst experience ever my car stopped in th...  \n",
      "1  the part were defective and the dealer did not...  \n",
      "2  the dealer insulted me when i asked about the ...  \n",
      "3  the worst experience ever my car stopped in th...  \n",
      "4  i wa scammed by the dealer and they refused to...  \n"
     ]
    }
   ],
   "source": [
    "# we are intializing the lemmatizer and stopwords now \n",
    "stop_words=set(stopwords.words('english'))\n",
    "stop_words\n",
    "# initializing the lemmatizer \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words=text.split()  # this splits the sentences into words\n",
    "    words=[lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['processed_review'] = df['cleaned_review'].apply(preprocess_text)\n",
    "print(df[['cleaned_review', 'processed_review']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The worst experience ever! My car stopped in t...</td>\n",
       "      <td>Highly Frustrated</td>\n",
       "      <td>the worst experience ever my car stopped in th...</td>\n",
       "      <td>the worst experience ever my car stopped in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The parts were defective, and the dealer did n...</td>\n",
       "      <td>Highly Frustrated</td>\n",
       "      <td>the parts were defective and the dealer did no...</td>\n",
       "      <td>the part were defective and the dealer did not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The dealer insulted me when I asked about the ...</td>\n",
       "      <td>Highly Frustrated</td>\n",
       "      <td>the dealer insulted me when i asked about the ...</td>\n",
       "      <td>the dealer insulted me when i asked about the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The worst experience ever! My car stopped in t...</td>\n",
       "      <td>Highly Frustrated</td>\n",
       "      <td>the worst experience ever my car stopped in th...</td>\n",
       "      <td>the worst experience ever my car stopped in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was scammed by the dealer, and they refused ...</td>\n",
       "      <td>Highly Frustrated</td>\n",
       "      <td>i was scammed by the dealer and they refused t...</td>\n",
       "      <td>i wa scammed by the dealer and they refused to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>The delivery was late, and I had to follow up ...</td>\n",
       "      <td>Average</td>\n",
       "      <td>the delivery was late and i had to follow up m...</td>\n",
       "      <td>the delivery wa late and i had to follow up mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>They forgot to provide a user manual at delivery.</td>\n",
       "      <td>Average</td>\n",
       "      <td>they forgot to provide a user manual at delivery</td>\n",
       "      <td>they forgot to provide a user manual at delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>The seat comfort is average, not as good as ex...</td>\n",
       "      <td>Average</td>\n",
       "      <td>the seat comfort is average not as good as exp...</td>\n",
       "      <td>the seat comfort is average not a good a expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>The dealer was not very responsive, but eventu...</td>\n",
       "      <td>Average</td>\n",
       "      <td>the dealer was not very responsive but eventua...</td>\n",
       "      <td>the dealer wa not very responsive but eventual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>The infotainment system has minor bugs, but no...</td>\n",
       "      <td>Average</td>\n",
       "      <td>the infotainment system has minor bugs but not...</td>\n",
       "      <td>the infotainment system ha minor bug but nothi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review          Sentiment  \\\n",
       "0      The worst experience ever! My car stopped in t...  Highly Frustrated   \n",
       "1      The parts were defective, and the dealer did n...  Highly Frustrated   \n",
       "2      The dealer insulted me when I asked about the ...  Highly Frustrated   \n",
       "3      The worst experience ever! My car stopped in t...  Highly Frustrated   \n",
       "4      I was scammed by the dealer, and they refused ...  Highly Frustrated   \n",
       "...                                                  ...                ...   \n",
       "14995  The delivery was late, and I had to follow up ...            Average   \n",
       "14996  They forgot to provide a user manual at delivery.            Average   \n",
       "14997  The seat comfort is average, not as good as ex...            Average   \n",
       "14998  The dealer was not very responsive, but eventu...            Average   \n",
       "14999  The infotainment system has minor bugs, but no...            Average   \n",
       "\n",
       "                                          cleaned_review  \\\n",
       "0      the worst experience ever my car stopped in th...   \n",
       "1      the parts were defective and the dealer did no...   \n",
       "2      the dealer insulted me when i asked about the ...   \n",
       "3      the worst experience ever my car stopped in th...   \n",
       "4      i was scammed by the dealer and they refused t...   \n",
       "...                                                  ...   \n",
       "14995  the delivery was late and i had to follow up m...   \n",
       "14996   they forgot to provide a user manual at delivery   \n",
       "14997  the seat comfort is average not as good as exp...   \n",
       "14998  the dealer was not very responsive but eventua...   \n",
       "14999  the infotainment system has minor bugs but not...   \n",
       "\n",
       "                                        processed_review  \n",
       "0      the worst experience ever my car stopped in th...  \n",
       "1      the part were defective and the dealer did not...  \n",
       "2      the dealer insulted me when i asked about the ...  \n",
       "3      the worst experience ever my car stopped in th...  \n",
       "4      i wa scammed by the dealer and they refused to...  \n",
       "...                                                  ...  \n",
       "14995  the delivery wa late and i had to follow up mu...  \n",
       "14996   they forgot to provide a user manual at delivery  \n",
       "14997  the seat comfort is average not a good a expected  \n",
       "14998  the dealer wa not very responsive but eventual...  \n",
       "14999  the infotainment system ha minor bug but nothi...  \n",
       "\n",
       "[15000 rows x 4 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review              0\n",
       "Sentiment           0\n",
       "cleaned_review      0\n",
       "processed_review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: They dealer didn't reply properly and hit me  -> Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_custom_sentiment(text):\n",
    "    polarity = sia.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "    if polarity < -0.3:\n",
    "        return \"Frustrated\"\n",
    "    elif -0.3 <= polarity < 0.1:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "#df['sentiment']= df['Review'].apply(get_custom_sentiment)\n",
    "# Test Example\n",
    "test_text = \"They dealer didn't reply properly and hit me \"\n",
    "print(f\"Text: {test_text} -> Sentiment: {get_custom_sentiment(test_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['sentiment_encoded']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Mine is a 99, the last year for the convertible. I love it. I have had no mechanical problems and I am up to 85K miles. My only complaints are the drink holders are too small and there is a lot of noise inside the car. But that is usually a problem in convertibles.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Review'][18731]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Added 91 additional frustrated reviews.\n"
     ]
    }
   ],
   "source": [
    "'''# Generate 200 additional frustrated reviews\n",
    "extra_frustrated_reviews = [\n",
    "    \"The service was absolutely terrible! They kept delaying my request.\",\n",
    "    \"I waited for hours, and still, no one attended to my issue. Completely useless!\",\n",
    "    \"They promised a replacement but kept giving excuses. Never trusting them again!\",\n",
    "    \"Why is everything so slow? I have been calling customer support for days!\",\n",
    "    \"My issue was ignored! They donâ€™t even bother to respond to emails.\",\n",
    "    \"They wasted my time, kept transferring my call from one department to another!\",\n",
    "    \"I was charged extra for a service I never received. Total scam!\",\n",
    "    \"No proper communication! I was left clueless about my order status.\",\n",
    "    \"Their app crashes all the time, and support is of no help!\",\n",
    "    \"Their customer support is a joke. They just copy-paste responses.\",\n",
    "    \"I had high expectations, but they completely let me down.\",\n",
    "    \"They lost my documents and didnâ€™t even apologize. Unacceptable!\",\n",
    "    \"I feel cheated! They misled me about their policies.\",\n",
    "    \"No updates, no follow-ups. They simply donâ€™t care about customers.\",\n",
    "    \"I had to call multiple times just to get a simple answer. Frustrating!\",\n",
    "    \"This is the worst experience Iâ€™ve had with any company.\",\n",
    "    \"Their attitude was rude and unprofessional. Never again!\",\n",
    "    \"False promises! They said delivery would take 2 days, but it's been weeks!\",\n",
    "    \"Terrible quality! Broke down within days, and now they refuse to help.\",\n",
    "    \"They just ignore complaints. Absolutely worthless service.\",\n",
    "    \"I regret ever choosing this brand. Itâ€™s a complete waste of money!\",\n",
    "    \"Their manager spoke to me like I was an idiot. Disrespectful!\",\n",
    "    \"The system overcharged me, and no one is willing to refund my money!\",\n",
    "    \"The technician was clueless. Made things worse instead of fixing them!\",\n",
    "    \"Took my money but didn't deliver what was promised. Fraudulent company!\",\n",
    "    \"They claim 24/7 support, but I was on hold for 2 hours!\",\n",
    "    \"Their chatbot is useless! It keeps giving the same responses.\",\n",
    "    \"I've had enough of their constant excuses. Totally unreliable!\",\n",
    "    \"They say they care about customers, but their actions prove otherwise!\",\n",
    "    \"The website is full of bugs. I can't even place an order properly!\",\n",
    "    \"I called multiple times, and each time they transferred me to someone else!\",\n",
    "    \"They sent me the wrong item, and now they refuse to exchange it!\",\n",
    "    \"I can't believe they treat customers like this. Absolutely disrespectful!\",\n",
    "    \"They ruined my entire plan with their inefficiency!\",\n",
    "    \"I had to fight to get a refund. They do everything to avoid paying!\",\n",
    "    \"Delivery was delayed multiple times, and they kept lying about it!\",\n",
    "    \"They never stick to their promises. Very unreliable company!\",\n",
    "    \"Their staff is untrained. They donâ€™t know what they are doing!\",\n",
    "    \"I asked for help, but they kept pushing me to buy more stuff!\",\n",
    "    \"The driver was rude and refused to deliver to my house!\",\n",
    "    \"I paid extra for express delivery, and it still arrived late!\",\n",
    "    \"They changed the price after I placed the order. Total scam!\",\n",
    "    \"The return process is a nightmare. They make it impossible to get a refund!\",\n",
    "    \"I was treated horribly. I felt completely disrespected!\",\n",
    "    \"I donâ€™t understand how they are still in business!\",\n",
    "    \"I submitted multiple tickets, but no one even responds!\",\n",
    "    \"I wasted so much time dealing with their incompetence!\",\n",
    "    \"They keep making promises but never follow through!\",\n",
    "    \"They canceled my order without telling me. Now I have to wait weeks for a refund!\",\n",
    "    \"Their online system is a mess. It doesn't even work properly!\",\n",
    "    \"They refuse to acknowledge their own mistakes!\",\n",
    "    \"I had to explain my issue to 5 different people before getting a useless response!\",\n",
    "    \"They overbooked and left me without a service I paid for!\",\n",
    "    \"They donâ€™t respect deadlines at all. Completely unprofessional!\",\n",
    "    \"Every time I try to talk to someone, I get a different answer!\",\n",
    "    \"I canâ€™t believe I have to chase them just to get what I paid for!\",\n",
    "    \"They keep saying 'sorry,' but nothing actually improves!\",\n",
    "    \"They ignore customer feedback and keep making the same mistakes!\",\n",
    "    \"Iâ€™ve been dealing with this issue for weeks, and still no solution!\",\n",
    "    \"They keep lying to customers to cover up their incompetence!\",\n",
    "    \"Their agents are rude and act like they donâ€™t care at all!\",\n",
    "    \"They wasted my entire day with their delays!\",\n",
    "    \"Their warranty policy is just a scam to avoid responsibility!\",\n",
    "    \"Their refund policy is designed to trap customers into not getting their money back!\",\n",
    "    \"They closed my complaint without even resolving it!\",\n",
    "    \"No one takes ownership of customer issues here!\",\n",
    "    \"The product description was misleading. I got something completely different!\",\n",
    "    \"They deleted my negative review instead of addressing my concerns!\",\n",
    "    \"The live chat agent just disconnected when I asked for a refund!\",\n",
    "    \"This company has no respect for its customers at all!\",\n",
    "    \"They refuse to compensate me for their mistake!\",\n",
    "    \"They keep blaming external factors instead of fixing their poor service!\",\n",
    "    \"They trick people into subscribing and then make it impossible to cancel!\",\n",
    "    \"Worst experience ever! I wish I had read the reviews before ordering!\",\n",
    "    \"They overpromised and underdelivered. I feel completely misled!\",\n",
    "    \"Their system charged me twice, and they wonâ€™t acknowledge it!\",\n",
    "    \"They are experts at giving excuses, not solving problems!\",\n",
    "    \"No transparency in pricing! They add hidden fees at checkout!\",\n",
    "    \"Their shipping policy is a joke. It takes forever to receive anything!\",\n",
    "    \"They act as if they are doing customers a favor instead of providing a service!\",\n",
    "    \"I will never recommend this company to anyone!\",\n",
    "    \"They have no regard for customer satisfaction whatsoever!\",\n",
    "    \"They sent me a defective product and now refuse to replace it!\",\n",
    "    \"They force you to go through ridiculous procedures just to get basic support!\",\n",
    "    \"Every time I call, I have to start the process from scratch!\",\n",
    "    \"Their CEO should be ashamed of how this company operates!\",\n",
    "    \"They take your money and then stop responding to complaints!\",\n",
    "    \"Their driver refused to deliver because it was 'too far'! What a joke!\",\n",
    "    \"I had to escalate the issue to a legal team to get my refund!\",\n",
    "    \"The worst customer experience I have ever had in my life!\",\n",
    "    \"They keep saying 'weâ€™ll look into it' but never actually do anything!\",\n",
    "]\n",
    "\n",
    "# Assign \"Frustrated\" sentiment to these reviews\n",
    "extra_sentiments = [\"Frustrated\"] * len(extra_frustrated_reviews)\n",
    "\n",
    "# Convert to DataFrame and append\n",
    "extra_data = pd.DataFrame({\"processed_review\": extra_frustrated_reviews, \"sentiment\": extra_sentiments})\n",
    "df = pd.concat([df, extra_data], ignore_index=True)\n",
    "\n",
    "print(f\"âœ… Added {len(extra_frustrated_reviews)} additional frustrated reviews.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Score: -1.0\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "review = \"The dealer was abusive, the car broke after delivery, it was very bad, didn't expect this from Toyota!\"\n",
    "sentiment = TextBlob(review).sentiment.polarity\n",
    "print(\"Sentiment Score:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frustrated</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highly Frustrated</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Review  cleaned_review  processed_review\n",
       "Sentiment                                                  \n",
       "Average              5000            5000              5000\n",
       "Frustrated           5000            5000              5000\n",
       "Highly Frustrated    5000            5000              5000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Sentiment\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive      13912\n",
       "Frustrated     2691\n",
       "Neutral        1352\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Frustrated    [my awd previa is the third one that i have ow...\n",
       "Neutral       [, , fun to drive have very problem, i purchas...\n",
       "Positive      [there is no way back enjoy what you have, st ...\n",
       "Name: processed_review, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sentiment\")['processed_review'].apply(list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Categorizing Reviews into Service, Parts, and Others**  \n",
    "Now, we will classify reviews into three categories:  \n",
    "1ï¸âƒ£ **Service-related** (e.g., repair, maintenance)  \n",
    "2ï¸âƒ£ **Parts-related** (e.g., engine, battery)  \n",
    "3ï¸âƒ£ **Others** (everything else)  \n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ“ Steps to Implement**  \n",
    "âœ… Define **keyword lists** for Service & Parts.  \n",
    "âœ… Check if a review contains **any keyword** from these lists.  \n",
    "âœ… Assign a category based on **matched keywords**.  \n",
    "âœ… Store the category in a new column called `\"category\"`.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ What This Does?**\n",
    "- Checks if **service-related** words exist â†’ Assigns **\"Service\"**  \n",
    "- Checks if **parts-related** words exist â†’ Assigns **\"Parts\"**  \n",
    "- If neither â†’ Assigns **\"Others\"**  \n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ” Next Step: Find New Frequent Words in \"Others\"**\n",
    "Once we categorize the existing reviews, we can check **what words are commonly appearing in \"Others\"** (in case there are new complaints that need a new category).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Others     9489\n",
       "Service    4514\n",
       "Parts       997\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Keywords for Classification\n",
    "service_keywords = [\"repair\", \"maintenance\", \"delay\", \"service\", \"issue\", \"problem\", \"technician\"]\n",
    "parts_keywords = [\"engine\", \"battery\", \"brake\", \"wheels\", \"tyre\", \"oil\", \"transmission\"]\n",
    "\n",
    "# Function to Assign Categories\n",
    "def categorize_review(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    if any(word in text for word in service_keywords):\n",
    "        return \"Service\"\n",
    "    elif any(word in text for word in parts_keywords):\n",
    "        return \"Parts\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "# Apply the Function to Categorize Reviews\n",
    "df['category'] = df['processed_review'].apply(categorize_review)\n",
    "\n",
    "# âœ… Check Category Distribution\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Identifying New Frequent Words in \"Others\" Category**  \n",
    "Since some reviews are classified as **\"Others\"**, we should check for **frequent words** in them. This will help us identify:  \n",
    "âœ… **New complaint trends** (e.g., a recurring issue with a new car part).  \n",
    "âœ… **Missing keywords** that should be added to the \"Service\" or \"Parts\" category.  \n",
    "âœ… **Potential new categories** if a large number of reviews mention the same issue.  \n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ“Œ What This Does?**\n",
    "1ï¸âƒ£ Filters out only reviews in the `\"Others\"` category.  \n",
    "2ï¸âƒ£ Splits reviews into individual words.  \n",
    "3ï¸âƒ£ Counts the most **frequent** words.  \n",
    "4ï¸âƒ£ Shows the **top 20 words** appearing in `\"Others\"` reviews.  \n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ” Next Step: Analyze Results**\n",
    "- If certain words appear **frequently**, we can **add them** to the `service_keywords` or `parts_keywords` list.  \n",
    "- If a **new issue** emerges, we might need a **new category**.  \n",
    "\n",
    "Run this and **share the top words** so we can refine the classification! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>10444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>4034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dealer</td>\n",
       "      <td>3461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>3033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wa</td>\n",
       "      <td>2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>slight</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>noise</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>while</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>driving</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>annoying</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Count\n",
       "0        the  10444\n",
       "1        and   4034\n",
       "2     dealer   3461\n",
       "3         to   3033\n",
       "4         wa   2466\n",
       "..       ...    ...\n",
       "65    slight    504\n",
       "66     noise    504\n",
       "67     while    504\n",
       "68   driving    504\n",
       "69  annoying    504\n",
       "\n",
       "[70 rows x 2 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Filter \"Others\" category reviews\n",
    "others_reviews = df[df['category'] == \"Others\"]['processed_review']\n",
    "\n",
    "# Tokenize words\n",
    "all_words = \" \".join(others_reviews).split()\n",
    "\n",
    "# Get the most common words\n",
    "word_counts = Counter(all_words)\n",
    "common_words = word_counts.most_common(70)  # Get top 20 words\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "common_words_df = pd.DataFrame(common_words, columns=[\"Word\", \"Count\"])\n",
    "\n",
    "# Display the top words\n",
    "common_words_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import train_test_split\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.metrics import classification_report, accuracy_score\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n# Encode Category Labels (Service = 0, Parts = 1, Others = 2)\\nlabel_encoder = LabelEncoder()\\ndf[\"category_encoded\"] = label_encoder.fit_transform(df[\"category\"])\\n\\n# Train-Test Split (80% Train, 20% Test)\\nX_train, X_test, y_train, y_test = train_test_split(\\n    df[\"processed_review\"], df[\"category_encoded\"], test_size=0.2, random_state=42, stratify=df[\"category_encoded\"]\\n)\\n\\n# TF-IDF Vectorization\\ntfidf = TfidfVectorizer(max_features=5000, stop_words=\\'english\\')\\nX_train_tfidf = tfidf.fit_transform(X_train)\\nX_test_tfidf = tfidf.transform(X_test)\\n\\n# Initialize Models\\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\\nsvm_model = SVC(kernel=\\'linear\\', probability=True, random_state=42)\\nnb_model = MultinomialNB()\\n\\n# Train & Evaluate Models\\nmodels = {\"Random Forest\": rf_model, \"SVM\": svm_model, \"NaÃ¯ve Bayes\": nb_model}\\nfor name, model in models.items():\\n    print(f\"\\nğŸ”¹ Training {name}...\")\\n    model.fit(X_train_tfidf, y_train)\\n    y_pred = model.predict(X_test_tfidf)\\n    \\n    print(f\"\\nâœ… Results for {name}:\")\\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\\n    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode Category Labels (Service = 0, Parts = 1, Others = 2)\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"category_encoded\"] = label_encoder.fit_transform(df[\"category\"])\n",
    "\n",
    "# Train-Test Split (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"processed_review\"], df[\"category_encoded\"], test_size=0.2, random_state=42, stratify=df[\"category_encoded\"]\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Initialize Models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train & Evaluate Models\n",
    "models = {\"Random Forest\": rf_model, \"SVM\": svm_model, \"NaÃ¯ve Bayes\": nb_model}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nğŸ”¹ Training {name}...\")\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    print(f\"\\nâœ… Results for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… SVM Sentiment Classification Results:\n",
      "Accuracy: 1.0000\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          Average       1.00      1.00      1.00      1000\n",
      "       Frustrated       1.00      1.00      1.00      1000\n",
      "Highly Frustrated       1.00      1.00      1.00      1000\n",
      "\n",
      "         accuracy                           1.00      3000\n",
      "        macro avg       1.00      1.00      1.00      3000\n",
      "     weighted avg       1.00      1.00      1.00      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode Sentiment Labels (Negative = 0, Neutral = 1, Positive = 2)\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"sentiment_encoded\"] = label_encoder.fit_transform(df[\"Sentiment\"])\n",
    "\n",
    "# Train-Test Split (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"processed_review\"], df[\"sentiment_encoded\"], test_size=0.2, random_state=42, stratify=df[\"sentiment_encoded\"]\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train SVM for Sentiment Classification\n",
    "svm_model = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Model Evaluation\n",
    "print(f\"\\nâœ… SVM Sentiment Classification Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Highly Frustrated\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review_text):\n",
    "    review_text_processed = preprocess_text(review_text)  # Apply preprocessing\n",
    "    review_tfidf = tfidf.transform([review_text_processed])  # Convert to TF-IDF\n",
    "    predicted_label = svm_model.predict(review_tfidf)  # Predict sentiment\n",
    "    sentiment = label_encoder.inverse_transform(predicted_label)  # Decode label\n",
    "    return sentiment[0]\n",
    "\n",
    "# Example\n",
    "new_review = \"dealer called me a b\"\n",
    "print(\"Predicted Sentiment:\", predict_sentiment(new_review))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to see the negative words it trained on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Incorrect Negative Words: ['slow', 'unresponsive', 'repair', 'delayed', 'month', 'acknowledge', 'scratch', 'car', 'frequent', 'helpful']\n",
      "âœ… Correct Negative Words: ['bug', 'overall', 'okay', 'driving', 'noise', 'annoying', 'slight', 'rude', 'unhelpful', 'wa']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get feature names from TF-IDF\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "svm_coefficients = svm_model.coef_.toarray()\n",
    "\n",
    "# Extract top negative words\n",
    "neg_class_idx = 0  # Assuming 0 = Negative\n",
    "top_negative_words = [feature_names[i] for i in svm_coefficients[neg_class_idx].argsort()[:10]]  # Incorrect\n",
    "top_corrected_negative_words = [feature_names[i] for i in svm_coefficients[neg_class_idx].argsort()[-10:]]  # Corrected\n",
    "\n",
    "print(\"âŒ Incorrect Negative Words:\", top_negative_words)\n",
    "print(\"âœ… Correct Negative Words:\", top_corrected_negative_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model implementaion starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Complete! Ready for Next Step.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Set parameters\n",
    "MAX_VOCAB_SIZE = 10000  # Limit vocabulary size\n",
    "MAX_SEQUENCE_LENGTH = 100  # Max words per review\n",
    "EMBEDDING_DIM = 100  # Embedding vector size\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"cleaned_review\"])\n",
    "\n",
    "# Convert text to sequences\n",
    "X = tokenizer.texts_to_sequences(df[\"cleaned_review\"])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_padded = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Encode sentiment labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"sentiment_encoded\"] = label_encoder.fit_transform(df[\"Sentiment\"])\n",
    "y = np.array(df[\"sentiment_encoded\"])\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Data Preparation Complete! Ready for Next Step.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Building the BiLSTM Model.**  \n",
    "\n",
    "### **Step 2: Define the BiLSTM Model**\n",
    "Hereâ€™s what weâ€™ll do:\n",
    "- Use an **Embedding Layer** to convert words into dense vectors.\n",
    "- Add a **Bidirectional LSTM Layer** to capture dependencies from both past and future words.\n",
    "- Use a **Dense Layer** with `softmax` activation for classification.\n",
    "\n",
    "Run the following code:  \n",
    "\n",
    "\n",
    "### **Whatâ€™s Next?**\n",
    "âœ… If this runs fine, weâ€™ll move to **Step 3: Training the Model.**  \n",
    "Let me know if there are any issues! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\New folder\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_6 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional_7 (\u001b[38;5;33mBidirectional\u001b[0m) â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the BiLSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),  # BiLSTM Layer\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Bidirectional(LSTM(32)),  # Another BiLSTM Layer\n",
    "    Dense(32, activation='relu'),  # Fully connected layer\n",
    "    Dropout(0.2),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the tokenizer with a vocabulary size\n",
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "# âœ… Convert all values to strings and handle NaNs\n",
    "X_train = X_train.astype(str).tolist() if isinstance(X_train, pd.Series) else [str(x) for x in X_train]\n",
    "X_test = X_test.astype(str).tolist() if isinstance(X_test, pd.Series) else [str(x) for x in X_test]\n",
    "\n",
    "# âœ… Fit tokenizer on training data\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# âœ… Convert text into sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print(\"âœ… Tokenization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Padding complete. Shapes: (13500, 100) (1500, 100)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 100  \n",
    "\n",
    "# Pad the sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(\"âœ… Padding complete. Shapes:\", X_train_padded.shape, X_test_padded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 76ms/step - accuracy: 0.8735 - loss: 0.2954 - val_accuracy: 1.0000 - val_loss: 4.7918e-04\n",
      "Epoch 2/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 2.1004e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 180ms/step - accuracy: 0.9998 - loss: 7.4144e-04 - val_accuracy: 1.0000 - val_loss: 5.1988e-06\n",
      "Epoch 4/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 183ms/step - accuracy: 1.0000 - loss: 2.3805e-04 - val_accuracy: 1.0000 - val_loss: 1.8137e-06\n",
      "Epoch 5/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 194ms/step - accuracy: 1.0000 - loss: 1.3082e-04 - val_accuracy: 1.0000 - val_loss: 8.8302e-07\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_padded, y_train, \n",
    "                    validation_data=(X_test_padded, y_test), \n",
    "                    epochs=5, \n",
    "                    batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 8.4085e-07\n",
      "âœ… Test Accuracy: 1.0000\n",
      "âœ… Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, y_test)\n",
    "\n",
    "print(f\"âœ… Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"âœ… Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step\n",
      "                 Actual          Predicted\n",
      "570             Average            Average\n",
      "715          Frustrated         Frustrated\n",
      "1330  Highly Frustrated  Highly Frustrated\n",
      "815             Average            Average\n",
      "1426            Average            Average\n",
      "767   Highly Frustrated  Highly Frustrated\n",
      "1145         Frustrated         Frustrated\n",
      "200   Highly Frustrated  Highly Frustrated\n",
      "548          Frustrated         Frustrated\n",
      "1444         Frustrated         Frustrated\n",
      "1125            Average            Average\n",
      "16    Highly Frustrated  Highly Frustrated\n",
      "93    Highly Frustrated  Highly Frustrated\n",
      "1029            Average            Average\n",
      "475          Frustrated         Frustrated\n",
      "465             Average            Average\n",
      "84           Frustrated         Frustrated\n",
      "1422         Frustrated         Frustrated\n",
      "254          Frustrated         Frustrated\n",
      "1283            Average            Average\n",
      "696             Average            Average\n",
      "321             Average            Average\n",
      "1424            Average            Average\n",
      "139             Average            Average\n",
      "986          Frustrated         Frustrated\n",
      "8            Frustrated         Frustrated\n",
      "324   Highly Frustrated  Highly Frustrated\n",
      "1192  Highly Frustrated  Highly Frustrated\n",
      "582             Average            Average\n",
      "1405  Highly Frustrated  Highly Frustrated\n",
      "1482            Average            Average\n",
      "764             Average            Average\n",
      "932             Average            Average\n",
      "832             Average            Average\n",
      "898          Frustrated         Frustrated\n",
      "661   Highly Frustrated  Highly Frustrated\n",
      "1306         Frustrated         Frustrated\n",
      "959             Average            Average\n",
      "408          Frustrated         Frustrated\n",
      "496             Average            Average\n",
      "1109         Frustrated         Frustrated\n",
      "492          Frustrated         Frustrated\n",
      "237             Average            Average\n",
      "41    Highly Frustrated  Highly Frustrated\n",
      "1357  Highly Frustrated  Highly Frustrated\n",
      "776   Highly Frustrated  Highly Frustrated\n",
      "672   Highly Frustrated  Highly Frustrated\n",
      "211          Frustrated         Frustrated\n",
      "828             Average            Average\n",
      "20              Average            Average\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_probs = model.predict(X_test_padded)  # Probabilities\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)  # Get class labels\n",
    "\n",
    "# Convert encoded labels back to original sentiment categories\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "# Compare actual vs. predicted sentiments\n",
    "sample_df = pd.DataFrame({'Actual': y_test_labels, 'Predicted': y_pred_labels})\n",
    "print(sample_df.sample(50))  # Show random 10 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "Predicted Sentiment: Highly Frustrated\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(sentence, model, tokenizer, label_encoder, max_length=100):\n",
    "    # Tokenize and pad the sentence\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "    # Predict sentiment\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]  # Get class label\n",
    "\n",
    "    # Convert label back to original sentiment\n",
    "    return label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "# Example usage\n",
    "sentence = \"\"\n",
    "print(f\"Predicted Sentiment: {predict_sentiment(sentence, model, tokenizer, label_encoder)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_encoded</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "      <td>2691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "      <td>1352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "      <td>13912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Column1  Review_Date  Author_Name  Vehicle_Title  \\\n",
       "sentiment_encoded                                                     \n",
       "0                     2691         2691         2691           2691   \n",
       "1                     1352         1352         1352           1352   \n",
       "2                    13912        13912        13912          13912   \n",
       "\n",
       "                   Review_Title  Review  Rating  cleaned_review  \\\n",
       "sentiment_encoded                                                 \n",
       "0                          2691    2691    2691            2691   \n",
       "1                          1352    1352    1352            1352   \n",
       "2                         13912   13912   13912           13912   \n",
       "\n",
       "                   processed_review  sentiment  category  \n",
       "sentiment_encoded                                         \n",
       "0                              2691       2691      2691  \n",
       "1                              1352       1352      1352  \n",
       "2                             13912      13912     13912  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('sentiment_encoded').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column1               91\n",
       "Review_Date           91\n",
       "Author_Name           91\n",
       "Vehicle_Title         91\n",
       "Review_Title          93\n",
       "Review                91\n",
       "Rating               881\n",
       "cleaned_review        91\n",
       "processed_review       0\n",
       "sentiment              0\n",
       "category               0\n",
       "sentiment_encoded      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column1              0\n",
       "Review_Date          0\n",
       "Author_Name          0\n",
       "Vehicle_Title        0\n",
       "Review_Title         0\n",
       "Review               0\n",
       "Rating               0\n",
       "cleaned_review       0\n",
       "processed_review     0\n",
       "sentiment            0\n",
       "category             0\n",
       "sentiment_encoded    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
