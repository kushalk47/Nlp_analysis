{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas \n",
    "#!pip install nltk\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0        there is no way back, enjoy what you have .   \n",
      "1   1st 95 went over 300k before being totalled b...   \n",
      "2   Sold 86 Toyota Van 285K miles to be replaced ...   \n",
      "3   I have owned lots of vans, and the Previa is ...   \n",
      "4   My 1997 AWD Previa is the third one that I ha...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0           there is no way back enjoy what you have  \n",
      "1  st  went over k before being totalled by a tru...  \n",
      "2  sold  toyota van k miles to be replaced with  ...  \n",
      "3  i have owned lots of vans and the previa is fa...  \n",
      "4  my  awd previa is the third one that i have ow...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset (Replace with actual file path)\n",
    "df = pd.read_csv(\"real.csv\")  # Change to your file path\n",
    "\n",
    "# Drop rows where 'Review' is missing\n",
    "df = df.dropna(subset=['Review'])\n",
    "\n",
    "# Convert text to lowercase and remove special characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure it's a string\n",
    "        text = text.lower()  # Lowercasing\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "        return text.strip()\n",
    "    return \"\"\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_review'] = df['Review'].apply(clean_text)\n",
    "\n",
    "# Show first few rows to verify\n",
    "print(df[['Review', 'cleaned_review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have downloaded the ntlk lib from that now we are installing stopwords and lemmatizier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  \\\n",
      "0           there is no way back enjoy what you have   \n",
      "1  st  went over k before being totalled by a tru...   \n",
      "2  sold  toyota van k miles to be replaced with  ...   \n",
      "3  i have owned lots of vans and the previa is fa...   \n",
      "4  my  awd previa is the third one that i have ow...   \n",
      "\n",
      "                                    processed_review  \n",
      "0           there is no way back enjoy what you have  \n",
      "1  st went over k before being totalled by a truc...  \n",
      "2  sold toyota van k mile to be replaced with pre...  \n",
      "3  i have owned lot of van and the previa is far ...  \n",
      "4  my awd previa is the third one that i have own...  \n"
     ]
    }
   ],
   "source": [
    "# we are intializing the lemmatizer and stopwords now \n",
    "stop_words=set(stopwords.words('english'))\n",
    "stop_words\n",
    "# initializing the lemmatizer \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words=text.split()  # this splits the sentences into words\n",
    "    words=[lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['processed_review'] = df['cleaned_review'].apply(preprocess_text)\n",
    "print(df[['cleaned_review', 'processed_review']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    processed_review sentiment\n",
      "0           there is no way back enjoy what you have  Positive\n",
      "1  st went over k before being totalled by a truc...  Positive\n",
      "2  sold toyota van k mile to be replaced with pre...  Positive\n",
      "3  i have owned lot of van and the previa is far ...   Neutral\n",
      "4  my awd previa is the third one that i have own...   Neutral\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to get sentiment category\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity  # Ranges from -1 (negative) to +1 (positive)\n",
    "    if polarity < -0.1:\n",
    "        return \"Frustrated\"\n",
    "    elif -0.1 <= polarity < 0.1:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df['sentiment'] = df['processed_review'].apply(get_sentiment)\n",
    "\n",
    "# Show the first few results\n",
    "print(df[['processed_review', 'sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Frustrated</th>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "      <td>831</td>\n",
       "      <td>832</td>\n",
       "      <td>789</td>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4600</td>\n",
       "      <td>4601</td>\n",
       "      <td>4326</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>12842</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Column1  Review_Date  Author_Name  Vehicle_Title  Review_Title  \\\n",
       "sentiment                                                                    \n",
       "Frustrated      832          832          832            832           831   \n",
       "Neutral        4601         4601         4601           4601          4600   \n",
       "Positive      13314        13314        13314          13314         13314   \n",
       "\n",
       "            Review  Rating  cleaned_review  processed_review  \n",
       "sentiment                                                     \n",
       "Frustrated     832     789             832               832  \n",
       "Neutral       4601    4326            4601              4601  \n",
       "Positive     13314   12842           13314             13314  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sentiment\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Frustrated    [cant go wrong with this car they thought of e...\n",
       "Neutral       [i have owned lot of van and the previa is far...\n",
       "Positive      [there is no way back enjoy what you have, st ...\n",
       "Name: processed_review, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sentiment\")['processed_review'].apply(list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Categorizing Reviews into Service, Parts, and Others**  \n",
    "Now, we will classify reviews into three categories:  \n",
    "1️⃣ **Service-related** (e.g., repair, maintenance)  \n",
    "2️⃣ **Parts-related** (e.g., engine, battery)  \n",
    "3️⃣ **Others** (everything else)  \n",
    "\n",
    "---\n",
    "\n",
    "### **📝 Steps to Implement**  \n",
    "✅ Define **keyword lists** for Service & Parts.  \n",
    "✅ Check if a review contains **any keyword** from these lists.  \n",
    "✅ Assign a category based on **matched keywords**.  \n",
    "✅ Store the category in a new column called `\"category\"`.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **📌 What This Does?**\n",
    "- Checks if **service-related** words exist → Assigns **\"Service\"**  \n",
    "- Checks if **parts-related** words exist → Assigns **\"Parts\"**  \n",
    "- If neither → Assigns **\"Others\"**  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔍 Next Step: Find New Frequent Words in \"Others\"**\n",
    "Once we categorize the existing reviews, we can check **what words are commonly appearing in \"Others\"** (in case there are new complaints that need a new category).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Others     9571\n",
       "Service    6496\n",
       "Parts      2680\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Keywords for Classification\n",
    "service_keywords = [\"repair\", \"maintenance\", \"delay\", \"service\", \"issue\", \"problem\", \"technician\"]\n",
    "parts_keywords = [\"engine\", \"battery\", \"brake\", \"wheels\", \"tyre\", \"oil\", \"transmission\"]\n",
    "\n",
    "# Function to Assign Categories\n",
    "def categorize_review(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    if any(word in text for word in service_keywords):\n",
    "        return \"Service\"\n",
    "    elif any(word in text for word in parts_keywords):\n",
    "        return \"Parts\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "# Apply the Function to Categorize Reviews\n",
    "df['category'] = df['processed_review'].apply(categorize_review)\n",
    "\n",
    "# ✅ Check Category Distribution\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Identifying New Frequent Words in \"Others\" Category**  \n",
    "Since some reviews are classified as **\"Others\"**, we should check for **frequent words** in them. This will help us identify:  \n",
    "✅ **New complaint trends** (e.g., a recurring issue with a new car part).  \n",
    "✅ **Missing keywords** that should be added to the \"Service\" or \"Parts\" category.  \n",
    "✅ **Potential new categories** if a large number of reviews mention the same issue.  \n",
    "\n",
    "---\n",
    "\n",
    "### **📌 What This Does?**\n",
    "1️⃣ Filters out only reviews in the `\"Others\"` category.  \n",
    "2️⃣ Splits reviews into individual words.  \n",
    "3️⃣ Counts the most **frequent** words.  \n",
    "4️⃣ Shows the **top 20 words** appearing in `\"Others\"` reviews.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔍 Next Step: Analyze Results**\n",
    "- If certain words appear **frequently**, we can **add them** to the `service_keywords` or `parts_keywords` list.  \n",
    "- If a **new issue** emerges, we might need a **new category**.  \n",
    "\n",
    "Run this and **share the top words** so we can refine the classification! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>33719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>20997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>20754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i</td>\n",
       "      <td>19649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it</td>\n",
       "      <td>16049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>or</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>no</td>\n",
       "      <td>1434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>am</td>\n",
       "      <td>1413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>well</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>can</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Count\n",
       "0    the  33719\n",
       "1      a  20997\n",
       "2    and  20754\n",
       "3      i  19649\n",
       "4     it  16049\n",
       "..   ...    ...\n",
       "65    or   1436\n",
       "66    no   1434\n",
       "67    am   1413\n",
       "68  well   1397\n",
       "69   can   1369\n",
       "\n",
       "[70 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Filter \"Others\" category reviews\n",
    "others_reviews = df[df['category'] == \"Others\"]['processed_review']\n",
    "\n",
    "# Tokenize words\n",
    "all_words = \" \".join(others_reviews).split()\n",
    "\n",
    "# Get the most common words\n",
    "word_counts = Counter(all_words)\n",
    "common_words = word_counts.most_common(70)  # Get top 20 words\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "common_words_df = pd.DataFrame(common_words, columns=[\"Word\", \"Count\"])\n",
    "\n",
    "# Display the top words\n",
    "common_words_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training Random Forest...\n",
      "\n",
      "✅ Results for Random Forest:\n",
      "Accuracy: 0.9277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Others       0.95      0.99      0.97      1915\n",
      "       Parts       0.97      0.60      0.74       536\n",
      "     Service       0.89      0.97      0.93      1299\n",
      "\n",
      "    accuracy                           0.93      3750\n",
      "   macro avg       0.94      0.85      0.88      3750\n",
      "weighted avg       0.93      0.93      0.92      3750\n",
      "\n",
      "\n",
      "🔹 Training SVM...\n",
      "\n",
      "✅ Results for SVM:\n",
      "Accuracy: 0.9675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Others       0.95      1.00      0.98      1915\n",
      "       Parts       0.94      0.90      0.92       536\n",
      "     Service       1.00      0.95      0.97      1299\n",
      "\n",
      "    accuracy                           0.97      3750\n",
      "   macro avg       0.97      0.95      0.96      3750\n",
      "weighted avg       0.97      0.97      0.97      3750\n",
      "\n",
      "\n",
      "🔹 Training Naïve Bayes...\n",
      "\n",
      "✅ Results for Naïve Bayes:\n",
      "Accuracy: 0.7349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Others       0.75      0.93      0.83      1915\n",
      "       Parts       0.93      0.03      0.05       536\n",
      "     Service       0.71      0.74      0.72      1299\n",
      "\n",
      "    accuracy                           0.73      3750\n",
      "   macro avg       0.80      0.57      0.53      3750\n",
      "weighted avg       0.76      0.73      0.68      3750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode Category Labels (Service = 0, Parts = 1, Others = 2)\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"category_encoded\"] = label_encoder.fit_transform(df[\"category\"])\n",
    "\n",
    "# Train-Test Split (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"processed_review\"], df[\"category_encoded\"], test_size=0.2, random_state=42, stratify=df[\"category_encoded\"]\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Initialize Models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train & Evaluate Models\n",
    "models = {\"Random Forest\": rf_model, \"SVM\": svm_model, \"Naïve Bayes\": nb_model}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔹 Training {name}...\")\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    print(f\"\\n✅ Results for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Complete! Ready for Next Step.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Set parameters\n",
    "MAX_VOCAB_SIZE = 10000  # Limit vocabulary size\n",
    "MAX_SEQUENCE_LENGTH = 100  # Max words per review\n",
    "EMBEDDING_DIM = 100  # Embedding vector size\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"cleaned_review\"])\n",
    "\n",
    "# Convert text to sequences\n",
    "X = tokenizer.texts_to_sequences(df[\"cleaned_review\"])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_padded = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Encode sentiment labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"sentiment_encoded\"] = label_encoder.fit_transform(df[\"sentiment\"])\n",
    "y = np.array(df[\"sentiment_encoded\"])\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Data Preparation Complete! Ready for Next Step.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Building the BiLSTM Model.**  \n",
    "\n",
    "### **Step 2: Define the BiLSTM Model**\n",
    "Here’s what we’ll do:\n",
    "- Use an **Embedding Layer** to convert words into dense vectors.\n",
    "- Add a **Bidirectional LSTM Layer** to capture dependencies from both past and future words.\n",
    "- Use a **Dense Layer** with `softmax` activation for classification.\n",
    "\n",
    "Run the following code:  \n",
    "\n",
    "\n",
    "### **What’s Next?**\n",
    "✅ If this runs fine, we’ll move to **Step 3: Training the Model.**  \n",
    "Let me know if there are any issues! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the BiLSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),  # BiLSTM Layer\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Bidirectional(LSTM(32)),  # Another BiLSTM Layer\n",
    "    Dense(32, activation='relu'),  # Fully connected layer\n",
    "    Dropout(0.2),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X_test, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m X_test\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Fit tokenizer on training data\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(X_train)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Convert text into sequences\u001b[39;00m\n\u001b[0;32m     14\u001b[0m X_train_sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_train)\n",
      "File \u001b[1;32mc:\\anaconda3\\New folder\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\text.py:127\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 127\u001b[0m         text \u001b[38;5;241m=\u001b[39m [text_elem\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m text_elem \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the tokenizer with a vocabulary size\n",
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "# ✅ Convert all values to strings and handle NaNs\n",
    "X_train = X_train.astype(str).tolist() if isinstance(X_train, pd.Series) else [str(x) for x in X_train]\n",
    "X_test = X_test.astype(str).tolist() if isinstance(X_test, pd.Series) else [str(x) for x in X_test]\n",
    "\n",
    "# ✅ Fit tokenizer on training data\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# ✅ Convert text into sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print(\"✅ Tokenization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 100  \n",
    "\n",
    "# Pad the sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(\"✅ Padding complete. Shapes:\", X_train_padded.shape, X_test_padded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_padded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_padded, y_train, \n\u001b[0;32m      3\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(X_test_padded, y_test), \n\u001b[0;32m      4\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m      5\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_padded' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_padded, y_train, \n",
    "                    validation_data=(X_test_padded, y_test), \n",
    "                    epochs=5, \n",
    "                    batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
