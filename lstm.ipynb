{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas \n",
    "#!pip install nltk\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0        there is no way back, enjoy what you have .   \n",
      "1   1st 95 went over 300k before being totalled b...   \n",
      "2   Sold 86 Toyota Van 285K miles to be replaced ...   \n",
      "3   I have owned lots of vans, and the Previa is ...   \n",
      "4   My 1997 AWD Previa is the third one that I ha...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0           there is no way back enjoy what you have  \n",
      "1  st  went over k before being totalled by a tru...  \n",
      "2  sold  toyota van k miles to be replaced with  ...  \n",
      "3  i have owned lots of vans and the previa is fa...  \n",
      "4  my  awd previa is the third one that i have ow...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset (Replace with actual file path)\n",
    "df = pd.read_csv(\"real.csv\")  # Change to your file path\n",
    "\n",
    "# Drop rows where 'Review' is missing\n",
    "df = df.dropna(subset=['Review'])\n",
    "\n",
    "# Convert text to lowercase and remove special characters\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure it's a string\n",
    "        text = text.lower()  # Lowercasing\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "        return text.strip()\n",
    "    return \"\"\n",
    "\n",
    "# Apply cleaning\n",
    "df['cleaned_review'] = df['Review'].apply(clean_text)\n",
    "\n",
    "# Show first few rows to verify\n",
    "print(df[['Review', 'cleaned_review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have downloaded the ntlk lib from that now we are installing stopwords and lemmatizier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      cleaned_review  \\\n",
      "0           there is no way back enjoy what you have   \n",
      "1  st  went over k before being totalled by a tru...   \n",
      "2  sold  toyota van k miles to be replaced with  ...   \n",
      "3  i have owned lots of vans and the previa is fa...   \n",
      "4  my  awd previa is the third one that i have ow...   \n",
      "\n",
      "                                    processed_review  \n",
      "0           there is no way back enjoy what you have  \n",
      "1  st went over k before being totalled by a truc...  \n",
      "2  sold toyota van k mile to be replaced with pre...  \n",
      "3  i have owned lot of van and the previa is far ...  \n",
      "4  my awd previa is the third one that i have own...  \n"
     ]
    }
   ],
   "source": [
    "# we are intializing the lemmatizer and stopwords now \n",
    "stop_words=set(stopwords.words('english'))\n",
    "stop_words\n",
    "# initializing the lemmatizer \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words=text.split()  # this splits the sentences into words\n",
    "    words=[lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['processed_review'] = df['cleaned_review'].apply(preprocess_text)\n",
    "print(df[['cleaned_review', 'processed_review']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    processed_review sentiment\n",
      "0           there is no way back enjoy what you have  Positive\n",
      "1  st went over k before being totalled by a truc...  Positive\n",
      "2  sold toyota van k mile to be replaced with pre...  Positive\n",
      "3  i have owned lot of van and the previa is far ...   Neutral\n",
      "4  my awd previa is the third one that i have own...   Neutral\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to get sentiment category\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity  # Ranges from -1 (negative) to +1 (positive)\n",
    "    if polarity < -0.1:\n",
    "        return \"Frustrated\"\n",
    "    elif -0.1 <= polarity < 0.1:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df['sentiment'] = df['processed_review'].apply(get_sentiment)\n",
    "\n",
    "# Show the first few results\n",
    "print(df[['processed_review', 'sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 91 additional frustrated reviews.\n"
     ]
    }
   ],
   "source": [
    "# Generate 200 additional frustrated reviews\n",
    "extra_frustrated_reviews = [\n",
    "    \"The service was absolutely terrible! They kept delaying my request.\",\n",
    "    \"I waited for hours, and still, no one attended to my issue. Completely useless!\",\n",
    "    \"They promised a replacement but kept giving excuses. Never trusting them again!\",\n",
    "    \"Why is everything so slow? I have been calling customer support for days!\",\n",
    "    \"My issue was ignored! They don’t even bother to respond to emails.\",\n",
    "    \"They wasted my time, kept transferring my call from one department to another!\",\n",
    "    \"I was charged extra for a service I never received. Total scam!\",\n",
    "    \"No proper communication! I was left clueless about my order status.\",\n",
    "    \"Their app crashes all the time, and support is of no help!\",\n",
    "    \"Their customer support is a joke. They just copy-paste responses.\",\n",
    "    \"I had high expectations, but they completely let me down.\",\n",
    "    \"They lost my documents and didn’t even apologize. Unacceptable!\",\n",
    "    \"I feel cheated! They misled me about their policies.\",\n",
    "    \"No updates, no follow-ups. They simply don’t care about customers.\",\n",
    "    \"I had to call multiple times just to get a simple answer. Frustrating!\",\n",
    "    \"This is the worst experience I’ve had with any company.\",\n",
    "    \"Their attitude was rude and unprofessional. Never again!\",\n",
    "    \"False promises! They said delivery would take 2 days, but it's been weeks!\",\n",
    "    \"Terrible quality! Broke down within days, and now they refuse to help.\",\n",
    "    \"They just ignore complaints. Absolutely worthless service.\",\n",
    "    \"I regret ever choosing this brand. It’s a complete waste of money!\",\n",
    "    \"Their manager spoke to me like I was an idiot. Disrespectful!\",\n",
    "    \"The system overcharged me, and no one is willing to refund my money!\",\n",
    "    \"The technician was clueless. Made things worse instead of fixing them!\",\n",
    "    \"Took my money but didn't deliver what was promised. Fraudulent company!\",\n",
    "    \"They claim 24/7 support, but I was on hold for 2 hours!\",\n",
    "    \"Their chatbot is useless! It keeps giving the same responses.\",\n",
    "    \"I've had enough of their constant excuses. Totally unreliable!\",\n",
    "    \"They say they care about customers, but their actions prove otherwise!\",\n",
    "    \"The website is full of bugs. I can't even place an order properly!\",\n",
    "    \"I called multiple times, and each time they transferred me to someone else!\",\n",
    "    \"They sent me the wrong item, and now they refuse to exchange it!\",\n",
    "    \"I can't believe they treat customers like this. Absolutely disrespectful!\",\n",
    "    \"They ruined my entire plan with their inefficiency!\",\n",
    "    \"I had to fight to get a refund. They do everything to avoid paying!\",\n",
    "    \"Delivery was delayed multiple times, and they kept lying about it!\",\n",
    "    \"They never stick to their promises. Very unreliable company!\",\n",
    "    \"Their staff is untrained. They don’t know what they are doing!\",\n",
    "    \"I asked for help, but they kept pushing me to buy more stuff!\",\n",
    "    \"The driver was rude and refused to deliver to my house!\",\n",
    "    \"I paid extra for express delivery, and it still arrived late!\",\n",
    "    \"They changed the price after I placed the order. Total scam!\",\n",
    "    \"The return process is a nightmare. They make it impossible to get a refund!\",\n",
    "    \"I was treated horribly. I felt completely disrespected!\",\n",
    "    \"I don’t understand how they are still in business!\",\n",
    "    \"I submitted multiple tickets, but no one even responds!\",\n",
    "    \"I wasted so much time dealing with their incompetence!\",\n",
    "    \"They keep making promises but never follow through!\",\n",
    "    \"They canceled my order without telling me. Now I have to wait weeks for a refund!\",\n",
    "    \"Their online system is a mess. It doesn't even work properly!\",\n",
    "    \"They refuse to acknowledge their own mistakes!\",\n",
    "    \"I had to explain my issue to 5 different people before getting a useless response!\",\n",
    "    \"They overbooked and left me without a service I paid for!\",\n",
    "    \"They don’t respect deadlines at all. Completely unprofessional!\",\n",
    "    \"Every time I try to talk to someone, I get a different answer!\",\n",
    "    \"I can’t believe I have to chase them just to get what I paid for!\",\n",
    "    \"They keep saying 'sorry,' but nothing actually improves!\",\n",
    "    \"They ignore customer feedback and keep making the same mistakes!\",\n",
    "    \"I’ve been dealing with this issue for weeks, and still no solution!\",\n",
    "    \"They keep lying to customers to cover up their incompetence!\",\n",
    "    \"Their agents are rude and act like they don’t care at all!\",\n",
    "    \"They wasted my entire day with their delays!\",\n",
    "    \"Their warranty policy is just a scam to avoid responsibility!\",\n",
    "    \"Their refund policy is designed to trap customers into not getting their money back!\",\n",
    "    \"They closed my complaint without even resolving it!\",\n",
    "    \"No one takes ownership of customer issues here!\",\n",
    "    \"The product description was misleading. I got something completely different!\",\n",
    "    \"They deleted my negative review instead of addressing my concerns!\",\n",
    "    \"The live chat agent just disconnected when I asked for a refund!\",\n",
    "    \"This company has no respect for its customers at all!\",\n",
    "    \"They refuse to compensate me for their mistake!\",\n",
    "    \"They keep blaming external factors instead of fixing their poor service!\",\n",
    "    \"They trick people into subscribing and then make it impossible to cancel!\",\n",
    "    \"Worst experience ever! I wish I had read the reviews before ordering!\",\n",
    "    \"They overpromised and underdelivered. I feel completely misled!\",\n",
    "    \"Their system charged me twice, and they won’t acknowledge it!\",\n",
    "    \"They are experts at giving excuses, not solving problems!\",\n",
    "    \"No transparency in pricing! They add hidden fees at checkout!\",\n",
    "    \"Their shipping policy is a joke. It takes forever to receive anything!\",\n",
    "    \"They act as if they are doing customers a favor instead of providing a service!\",\n",
    "    \"I will never recommend this company to anyone!\",\n",
    "    \"They have no regard for customer satisfaction whatsoever!\",\n",
    "    \"They sent me a defective product and now refuse to replace it!\",\n",
    "    \"They force you to go through ridiculous procedures just to get basic support!\",\n",
    "    \"Every time I call, I have to start the process from scratch!\",\n",
    "    \"Their CEO should be ashamed of how this company operates!\",\n",
    "    \"They take your money and then stop responding to complaints!\",\n",
    "    \"Their driver refused to deliver because it was 'too far'! What a joke!\",\n",
    "    \"I had to escalate the issue to a legal team to get my refund!\",\n",
    "    \"The worst customer experience I have ever had in my life!\",\n",
    "    \"They keep saying 'we’ll look into it' but never actually do anything!\",\n",
    "]\n",
    "\n",
    "# Assign \"Frustrated\" sentiment to these reviews\n",
    "extra_sentiments = [\"Frustrated\"] * len(extra_frustrated_reviews)\n",
    "\n",
    "# Convert to DataFrame and append\n",
    "extra_data = pd.DataFrame({\"processed_review\": extra_frustrated_reviews, \"sentiment\": extra_sentiments})\n",
    "df = pd.concat([df, extra_data], ignore_index=True)\n",
    "\n",
    "print(f\"✅ Added {len(extra_frustrated_reviews)} additional frustrated reviews.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Score: -1.0\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "review = \"The dealer was abusive, the car broke after delivery, it was very bad, didn't expect this from Toyota!\"\n",
    "sentiment = TextBlob(review).sentiment.polarity\n",
    "print(\"Sentiment Score:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Author_Name</th>\n",
       "      <th>Vehicle_Title</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>processed_review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Frustrated</th>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "      <td>832</td>\n",
       "      <td>831</td>\n",
       "      <td>832</td>\n",
       "      <td>789</td>\n",
       "      <td>832</td>\n",
       "      <td>923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "      <td>4600</td>\n",
       "      <td>4601</td>\n",
       "      <td>4326</td>\n",
       "      <td>4601</td>\n",
       "      <td>4601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "      <td>12842</td>\n",
       "      <td>13314</td>\n",
       "      <td>13314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Column1  Review_Date  Author_Name  Vehicle_Title  Review_Title  \\\n",
       "sentiment                                                                    \n",
       "Frustrated      832          832          832            832           831   \n",
       "Neutral        4601         4601         4601           4601          4600   \n",
       "Positive      13314        13314        13314          13314         13314   \n",
       "\n",
       "            Review  Rating  cleaned_review  processed_review  \n",
       "sentiment                                                     \n",
       "Frustrated     832     789             832               923  \n",
       "Neutral       4601    4326            4601              4601  \n",
       "Positive     13314   12842           13314             13314  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sentiment\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive      13314\n",
       "Neutral        4601\n",
       "Frustrated      923\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Frustrated    [cant go wrong with this car they thought of e...\n",
       "Neutral       [i have owned lot of van and the previa is far...\n",
       "Positive      [there is no way back enjoy what you have, st ...\n",
       "Name: processed_review, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sentiment\")['processed_review'].apply(list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Categorizing Reviews into Service, Parts, and Others**  \n",
    "Now, we will classify reviews into three categories:  \n",
    "1️⃣ **Service-related** (e.g., repair, maintenance)  \n",
    "2️⃣ **Parts-related** (e.g., engine, battery)  \n",
    "3️⃣ **Others** (everything else)  \n",
    "\n",
    "---\n",
    "\n",
    "### **📝 Steps to Implement**  \n",
    "✅ Define **keyword lists** for Service & Parts.  \n",
    "✅ Check if a review contains **any keyword** from these lists.  \n",
    "✅ Assign a category based on **matched keywords**.  \n",
    "✅ Store the category in a new column called `\"category\"`.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **📌 What This Does?**\n",
    "- Checks if **service-related** words exist → Assigns **\"Service\"**  \n",
    "- Checks if **parts-related** words exist → Assigns **\"Parts\"**  \n",
    "- If neither → Assigns **\"Others\"**  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔍 Next Step: Find New Frequent Words in \"Others\"**\n",
    "Once we categorize the existing reviews, we can check **what words are commonly appearing in \"Others\"** (in case there are new complaints that need a new category).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Others     9646\n",
       "Service    6512\n",
       "Parts      2680\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Keywords for Classification\n",
    "service_keywords = [\"repair\", \"maintenance\", \"delay\", \"service\", \"issue\", \"problem\", \"technician\"]\n",
    "parts_keywords = [\"engine\", \"battery\", \"brake\", \"wheels\", \"tyre\", \"oil\", \"transmission\"]\n",
    "\n",
    "# Function to Assign Categories\n",
    "def categorize_review(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    if any(word in text for word in service_keywords):\n",
    "        return \"Service\"\n",
    "    elif any(word in text for word in parts_keywords):\n",
    "        return \"Parts\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "# Apply the Function to Categorize Reviews\n",
    "df['category'] = df['processed_review'].apply(categorize_review)\n",
    "\n",
    "# ✅ Check Category Distribution\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Identifying New Frequent Words in \"Others\" Category**  \n",
    "Since some reviews are classified as **\"Others\"**, we should check for **frequent words** in them. This will help us identify:  \n",
    "✅ **New complaint trends** (e.g., a recurring issue with a new car part).  \n",
    "✅ **Missing keywords** that should be added to the \"Service\" or \"Parts\" category.  \n",
    "✅ **Potential new categories** if a large number of reviews mention the same issue.  \n",
    "\n",
    "---\n",
    "\n",
    "### **📌 What This Does?**\n",
    "1️⃣ Filters out only reviews in the `\"Others\"` category.  \n",
    "2️⃣ Splits reviews into individual words.  \n",
    "3️⃣ Counts the most **frequent** words.  \n",
    "4️⃣ Shows the **top 20 words** appearing in `\"Others\"` reviews.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔍 Next Step: Analyze Results**\n",
    "- If certain words appear **frequently**, we can **add them** to the `service_keywords` or `parts_keywords` list.  \n",
    "- If a **new issue** emerges, we might need a **new category**.  \n",
    "\n",
    "Run this and **share the top words** so we can refine the classification! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>33728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>21012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>20770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i</td>\n",
       "      <td>19649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it</td>\n",
       "      <td>16053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>no</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>or</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>am</td>\n",
       "      <td>1413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>well</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>can</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word  Count\n",
       "0    the  33728\n",
       "1      a  21012\n",
       "2    and  20770\n",
       "3      i  19649\n",
       "4     it  16053\n",
       "..   ...    ...\n",
       "65    no   1440\n",
       "66    or   1436\n",
       "67    am   1413\n",
       "68  well   1397\n",
       "69   can   1369\n",
       "\n",
       "[70 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Filter \"Others\" category reviews\n",
    "others_reviews = df[df['category'] == \"Others\"]['processed_review']\n",
    "\n",
    "# Tokenize words\n",
    "all_words = \" \".join(others_reviews).split()\n",
    "\n",
    "# Get the most common words\n",
    "word_counts = Counter(all_words)\n",
    "common_words = word_counts.most_common(70)  # Get top 20 words\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "common_words_df = pd.DataFrame(common_words, columns=[\"Word\", \"Count\"])\n",
    "\n",
    "# Display the top words\n",
    "common_words_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import train_test_split\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.svm import SVC\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.metrics import classification_report, accuracy_score\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n# Encode Category Labels (Service = 0, Parts = 1, Others = 2)\\nlabel_encoder = LabelEncoder()\\ndf[\"category_encoded\"] = label_encoder.fit_transform(df[\"category\"])\\n\\n# Train-Test Split (80% Train, 20% Test)\\nX_train, X_test, y_train, y_test = train_test_split(\\n    df[\"processed_review\"], df[\"category_encoded\"], test_size=0.2, random_state=42, stratify=df[\"category_encoded\"]\\n)\\n\\n# TF-IDF Vectorization\\ntfidf = TfidfVectorizer(max_features=5000, stop_words=\\'english\\')\\nX_train_tfidf = tfidf.fit_transform(X_train)\\nX_test_tfidf = tfidf.transform(X_test)\\n\\n# Initialize Models\\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\\nsvm_model = SVC(kernel=\\'linear\\', probability=True, random_state=42)\\nnb_model = MultinomialNB()\\n\\n# Train & Evaluate Models\\nmodels = {\"Random Forest\": rf_model, \"SVM\": svm_model, \"Naïve Bayes\": nb_model}\\nfor name, model in models.items():\\n    print(f\"\\n🔹 Training {name}...\")\\n    model.fit(X_train_tfidf, y_train)\\n    y_pred = model.predict(X_test_tfidf)\\n    \\n    print(f\"\\n✅ Results for {name}:\")\\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\\n    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode Category Labels (Service = 0, Parts = 1, Others = 2)\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"category_encoded\"] = label_encoder.fit_transform(df[\"category\"])\n",
    "\n",
    "# Train-Test Split (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"processed_review\"], df[\"category_encoded\"], test_size=0.2, random_state=42, stratify=df[\"category_encoded\"]\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Initialize Models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train & Evaluate Models\n",
    "models = {\"Random Forest\": rf_model, \"SVM\": svm_model, \"Naïve Bayes\": nb_model}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔹 Training {name}...\")\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    print(f\"\\n✅ Results for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode Sentiment Labels (Negative = 0, Neutral = 1, Positive = 2)\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"sentiment_encoded\"] = label_encoder.fit_transform(df[\"sentiment\"])\n",
    "\n",
    "# Train-Test Split (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"processed_review\"], df[\"sentiment_encoded\"], test_size=0.2, random_state=42, stratify=df[\"sentiment_encoded\"]\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train SVM for Sentiment Classification\n",
    "svm_model = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Model Evaluation\n",
    "print(f\"\\n✅ SVM Sentiment Classification Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review_text):\n",
    "    review_text_processed = preprocess_text(review_text)  # Apply preprocessing\n",
    "    review_tfidf = tfidf.transform([review_text_processed])  # Convert to TF-IDF\n",
    "    predicted_label = svm_model.predict(review_tfidf)  # Predict sentiment\n",
    "    sentiment = label_encoder.inverse_transform(predicted_label)  # Decode label\n",
    "    return sentiment[0]\n",
    "\n",
    "# Example\n",
    "new_review = \" i had the worst exprience with the dealer and he used abusive language\"\n",
    "print(\"Predicted Sentiment:\", predict_sentiment(new_review))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to see the negative words it trained on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Incorrect Negative Words: ['great', 'love', 'nice', 'fine', 'best', 'new', 'better', 'good', 'far', 'light']\n",
      "✅ Correct Negative Words: ['wrong', 'awful', 'boring', 'annoying', 'poor', 'terrible', 'horrible', 'bad', 'worst', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get feature names from TF-IDF\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "svm_coefficients = svm_model.coef_.toarray()\n",
    "\n",
    "# Extract top negative words\n",
    "neg_class_idx = 0  # Assuming 0 = Negative\n",
    "top_negative_words = [feature_names[i] for i in svm_coefficients[neg_class_idx].argsort()[:10]]  # Incorrect\n",
    "top_corrected_negative_words = [feature_names[i] for i in svm_coefficients[neg_class_idx].argsort()[-10:]]  # Corrected\n",
    "\n",
    "print(\"❌ Incorrect Negative Words:\", top_negative_words)\n",
    "print(\"✅ Correct Negative Words:\", top_corrected_negative_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[0;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39mMAX_VOCAB_SIZE, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_review\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Convert text to sequences\u001b[39;00m\n\u001b[0;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_review\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\anaconda3\\New folder\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\text.py:133\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m         seq \u001b[38;5;241m=\u001b[39m text_to_word_sequence(\n\u001b[0;32m    134\u001b[0m             text,\n\u001b[0;32m    135\u001b[0m             filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters,\n\u001b[0;32m    136\u001b[0m             lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower,\n\u001b[0;32m    137\u001b[0m             split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit,\n\u001b[0;32m    138\u001b[0m         )\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer(text)\n",
      "File \u001b[1;32mc:\\anaconda3\\New folder\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\text.py:22\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 22\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m input_text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     24\u001b[0m translate_dict \u001b[38;5;241m=\u001b[39m {c: split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m filters}\n\u001b[0;32m     25\u001b[0m translate_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(translate_dict)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Set parameters\n",
    "MAX_VOCAB_SIZE = 10000  # Limit vocabulary size\n",
    "MAX_SEQUENCE_LENGTH = 100  # Max words per review\n",
    "EMBEDDING_DIM = 100  # Embedding vector size\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"cleaned_review\"])\n",
    "\n",
    "# Convert text to sequences\n",
    "X = tokenizer.texts_to_sequences(df[\"cleaned_review\"])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_padded = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Encode sentiment labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"sentiment_encoded\"] = label_encoder.fit_transform(df[\"sentiment\"])\n",
    "y = np.array(df[\"sentiment_encoded\"])\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Data Preparation Complete! Ready for Next Step.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Building the BiLSTM Model.**  \n",
    "\n",
    "### **Step 2: Define the BiLSTM Model**\n",
    "Here’s what we’ll do:\n",
    "- Use an **Embedding Layer** to convert words into dense vectors.\n",
    "- Add a **Bidirectional LSTM Layer** to capture dependencies from both past and future words.\n",
    "- Use a **Dense Layer** with `softmax` activation for classification.\n",
    "\n",
    "Run the following code:  \n",
    "\n",
    "\n",
    "### **What’s Next?**\n",
    "✅ If this runs fine, we’ll move to **Step 3: Training the Model.**  \n",
    "Let me know if there are any issues! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the BiLSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),  # BiLSTM Layer\n",
    "    Dropout(0.3),  # Dropout for regularization\n",
    "    Bidirectional(LSTM(32)),  # Another BiLSTM Layer\n",
    "    Dense(32, activation='relu'),  # Fully connected layer\n",
    "    Dropout(0.2),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the tokenizer with a vocabulary size\n",
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "\n",
    "# ✅ Convert all values to strings and handle NaNs\n",
    "X_train = X_train.astype(str).tolist() if isinstance(X_train, pd.Series) else [str(x) for x in X_train]\n",
    "X_test = X_test.astype(str).tolist() if isinstance(X_test, pd.Series) else [str(x) for x in X_test]\n",
    "\n",
    "# ✅ Fit tokenizer on training data\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# ✅ Convert text into sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print(\"✅ Tokenization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Padding complete. Shapes: (16872, 100) (1875, 100)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 100  \n",
    "\n",
    "# Pad the sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(\"✅ Padding complete. Shapes:\", X_train_padded.shape, X_test_padded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 96ms/step - accuracy: 0.7072 - loss: 0.7376 - val_accuracy: 0.7221 - val_loss: 0.6121\n",
      "Epoch 2/5\n",
      "\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 70ms/step - accuracy: 0.7730 - loss: 0.5232 - val_accuracy: 0.8123 - val_loss: 0.4720\n",
      "Epoch 3/5\n",
      "\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 69ms/step - accuracy: 0.8310 - loss: 0.3930 - val_accuracy: 0.8352 - val_loss: 0.4157\n",
      "Epoch 4/5\n",
      "\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 67ms/step - accuracy: 0.8837 - loss: 0.2888 - val_accuracy: 0.8373 - val_loss: 0.4055\n",
      "Epoch 5/5\n",
      "\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 67ms/step - accuracy: 0.9092 - loss: 0.2278 - val_accuracy: 0.8128 - val_loss: 0.4678\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_padded, y_train, \n",
    "                    validation_data=(X_test_padded, y_test), \n",
    "                    epochs=5, \n",
    "                    batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8060 - loss: 0.4778\n",
      "✅ Test Accuracy: 0.8128\n",
      "✅ Test Loss: 0.4678\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_padded, y_test)\n",
    "\n",
    "print(f\"✅ Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"✅ Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "        Actual   Predicted\n",
      "691    Neutral    Positive\n",
      "812   Positive    Positive\n",
      "125   Positive    Positive\n",
      "1078   Neutral     Neutral\n",
      "333    Neutral     Neutral\n",
      "1758   Neutral  Frustrated\n",
      "1682  Positive    Positive\n",
      "1053   Neutral     Neutral\n",
      "493   Positive    Positive\n",
      "381   Positive     Neutral\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_probs = model.predict(X_test_padded)  # Probabilities\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)  # Get class labels\n",
    "\n",
    "# Convert encoded labels back to original sentiment categories\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "# Compare actual vs. predicted sentiments\n",
    "sample_df = pd.DataFrame({'Actual': y_test_labels, 'Predicted': y_pred_labels})\n",
    "print(sample_df.sample(10))  # Show random 10 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Being in the tire business and an auto enthusiast, I was skeptical at first as I had looked at a Lexus product also. This car has a smooth ride, amazing back seat room, a powerful engine and just delivered an amazing 32.1 MPG on a recent interstate drive.  This auto is a \"well kept\" secret that needs to be unleashed on the automobile public who enjoy a smooth ride, volumes of interior room, etc. and is competitive with Lexus in value and features!'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Review'][381]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
